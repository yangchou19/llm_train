{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026ce81a",
   "metadata": {},
   "source": [
    "# 1. å¯¼å…¥ç¯å¢ƒ\n",
    "\n",
    "æœ¬èŠ‚å°†å¯¼å…¥è®­ç»ƒDeepSeek-7B-chatæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰Pythonåº“å’Œä¾èµ–åŒ…ï¼Œå¹¶è¿›è¡ŒåŸºç¡€çš„ç¯å¢ƒé…ç½®ã€‚\n",
    "\n",
    "### ä¸»è¦ä¾èµ–åº“è¯´æ˜\n",
    "\n",
    "#### æ ¸å¿ƒæœºå™¨å­¦ä¹ åº“\n",
    "- **transformers**: Hugging Faceçš„Transformersåº“ï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "- **torch**: PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›æ¨¡å‹è®­ç»ƒçš„åŸºç¡€åŠŸèƒ½\n",
    "- **datasets**: Hugging Faceçš„æ•°æ®é›†å¤„ç†åº“ï¼Œç”¨äºé«˜æ•ˆå¤„ç†è®­ç»ƒæ•°æ®\n",
    "- **peft**: Parameter-Efficient Fine-Tuningåº“ï¼Œæä¾›LoRAç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•\n",
    "\n",
    "#### æ•°æ®å¤„ç†åº“\n",
    "- **pandas**: æ•°æ®åˆ†æå’Œå¤„ç†åº“ï¼Œç”¨äºè¯»å–JSONæ•°æ®å¹¶è½¬æ¢æ ¼å¼\n",
    "- **numpy**: æ•°å€¼è®¡ç®—åº“ï¼ˆé€šè¿‡å…¶ä»–åº“é—´æ¥ä½¿ç”¨ï¼‰\n",
    "\n",
    "#### è®­ç»ƒç›¸å…³ç»„ä»¶\n",
    "- **AutoTokenizer**: è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨\n",
    "- **AutoModelForCausalLM**: è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹\n",
    "- **DataCollatorForSeq2Seq**: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„æ•°æ®æ•´ç†å™¨\n",
    "- **TrainingArguments**: è®­ç»ƒå‚æ•°é…ç½®ç±»\n",
    "- **Trainer**: Hugging Faceçš„è®­ç»ƒå™¨ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹\n",
    "- **GenerationConfig**: æ–‡æœ¬ç”Ÿæˆé…ç½®ç±»\n",
    "\n",
    "### ç¯å¢ƒè¦æ±‚\n",
    "- Python 3.8+\n",
    "- CUDA 11.8+ (ç”¨äºGPUè®­ç»ƒ)\n",
    "- è‡³å°‘24GB GPUå†…å­˜ (æ¨è24GB+)\n",
    "- è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨æ¨¡å‹å’Œæ•°æ®é›†\n",
    "- GPUé©±åŠ¨å’ŒCUDAå·¥å…·åŒ…å·²æ­£ç¡®å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fac949-4150-4091-b0c3-2968ab5e385c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/train/llm_train/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥ç›¸åº”çš„ä¾èµ–\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d05e5d-d14e-4f03-92be-9a9677d41918",
   "metadata": {},
   "source": [
    "# 2.è¯»å–å’Œå¤„ç†æ•°æ®é›†\n",
    "### 2.1 è¯»å–æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e098d9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦',\n",
       "  'ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦',\n",
       "  'ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦'],\n",
       " 'input': ['æˆ‘æƒ³é€èµµè€å¸ˆç”Ÿæ—¥ç¥ç¦,ä¸¥è‚ƒé£æ ¼', 'é€ç¥ç¦ç»™èµµè€å¸ˆæ˜¥èŠ‚,ä¸¥è‚ƒé£æ ¼', 'ç¥èµµè€å¸ˆå…ƒå®µèŠ‚å¿«ä¹,ä¸¥è‚ƒé£æ ¼'],\n",
       " 'output': ['å°Šæ•¬çš„èµµè€å¸ˆï¼Œå€¼æ­¤ç”Ÿè¾°ä¹‹é™…ï¼Œæ„¿å²æœˆå¦‚è¯—ï¼Œä¸ºæ‚¨å¸¦æ¥æ— å°½çš„å–œæ‚¦ä¸ç¾å¥½ï¼›æ„¿æ—¶å…‰èè‹’ï¼Œä¸ºæ‚¨ç•™ä¸‹çè´µçš„å›å¿†ä¸æ„Ÿæ‚Ÿã€‚æ„¿æ‚¨å¿«å¿«ä¹ä¹ï¼Œäº‹ä¸šæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œæ•™è¯²ä¹‹æ©æ¡ƒææ»¡å¤©ä¸‹ã€‚åœ¨è¿™å……æ»¡æ•¬æ„çš„æ—¶åˆ»ï¼Œæ­ç¥èµµè€å¸ˆç”Ÿæ—¥å¿«ä¹ï¼Œå¹¸ç¦å®‰åº·ï¼',\n",
       "  'èµµè€å¸ˆï¼Œå€¼æ­¤æ˜¥èŠ‚ä½³èŠ‚ä¹‹é™…ï¼Œæ­ç¥æ‚¨ç¦å¯¿å®‰åº·ï¼Œä¸‡äº‹å¦‚æ„ã€‚åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæ‚¨çš„è¾›å‹¤è€•è€˜ä¸ºåè¾ˆæ ‘ç«‹äº†æ¦œæ ·ï¼Œæ–°æ˜¥åˆ°æ¥ï¼Œæ„¿æ‚¨çš„ç”Ÿæ´»å¦‚è¯—å¦‚ç”»ï¼Œå·¥ä½œæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œç»§ç»­ä»¥æ‚¨çš„æ™ºæ…§å’Œçƒ­å¿±ï¼Œå¼•é¢†æˆ‘ä»¬å‰è¡Œã€‚å²æœˆé™å¥½ï¼Œæ„¿æ‚¨äº«å—æ¯ä¸€ä¸ªæ¸©é¦¨æ—¶åˆ»ï¼Œå¹¸ç¦å®‰åº·ï¼Œå–œæ‚¦æ— å¿§ã€‚',\n",
       "  'å°Šæ•¬çš„èµµè€å¸ˆï¼Œå…ƒå®µä½³èŠ‚è‡³ï¼Œæ„¿æ‚¨ç¦å¯¿å®‰åº·ï¼Œæ™ºæ…§å¦‚ç¯ã€‚æ„¿è¿™å›¢åœ†çš„ç¯ç«ï¼Œç…§äº®æ‚¨çš„äº‹ä¸šä¸å‰ç¨‹ï¼Œå¸¦æ¥æ›´å¤šå­¦æœ¯çš„è¾‰ç…Œä¸æˆå°±ã€‚åœ¨è¿™æ¸©é¦¨çš„æ—¶åˆ»ï¼Œæ„¿æ‚¨å®¶åº­ç¾æ»¡ï¼Œå¹¸ç¦é•¿å­˜ï¼Œå¦‚å…ƒå®µèˆ¬åœ†æ»¡ç”œèœœã€‚è°¨ç¥å…ƒå®µå¿«ä¹ï¼Œä¸‡äº‹å¦‚æ„ï¼']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å–å¯¹åº”çš„æ•°æ®\n",
    "file_path = './dataset/tianji-chinese/tianji-wishes-chinese-v0.1-format.json'\n",
    "df = pd.read_json(file_path)\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccb2c5",
   "metadata": {},
   "source": [
    "### 2.2 åŠ è½½åˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./deepseek-ai/deepseek-llm-7b-chat/', vocab_size=100000, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<ï½œbeginâ–ofâ–sentenceï½œ>', 'eos_token': '<ï½œendâ–ofâ–sentenceï½œ>', 'pad_token': '<ï½œendâ–ofâ–sentenceï½œ>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t100000: AddedToken(\"<ï½œbeginâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t100001: AddedToken(\"<ï½œendâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t100002: AddedToken(\"Ã¸\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100003: AddedToken(\"Ã¶\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100004: AddedToken(\"Ãº\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100005: AddedToken(\"Ã¿\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100006: AddedToken(\"Ãµ\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100007: AddedToken(\"Ã·\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100008: AddedToken(\"Ã»\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100009: AddedToken(\"Ã½\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100010: AddedToken(\"Ã€\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100011: AddedToken(\"Ã¹\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100012: AddedToken(\"Ã\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100013: AddedToken(\"Ã¾\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100014: AddedToken(\"Ã¼\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', use_fast=False, trust_remote_code=True)\n",
    "# è®¾ç½®å¡«å……æ–¹å‘ä¸ºå³ä¾§å¡«å……ï¼Œä¸ä¼šå½±å“æ¨¡å‹å¯¹åºåˆ—å¼€å¤´çš„ç†è§£\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c330e94",
   "metadata": {},
   "source": [
    "### 2.3 æ•°æ®æ ¼å¼åŒ–\n",
    "Lora è®­ç»ƒçš„æ•°æ®æ˜¯éœ€è¦ç»è¿‡æ ¼å¼åŒ–ã€ç¼–ç ä¹‹åå†è¾“å…¥ç»™æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå°†æ•°æ®ç¼–ç æˆå¤šç»´å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2503a5fa-9621-4495-9035-8e7ef6525691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2976 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2976/2976 [00:01<00:00, 2743.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2976\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llamaåˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"User: {example['instruction']+example['input']}\\n\\n\", add_special_tokens=False)  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens\n",
    "    response = tokenizer(f\"Assistant: {example['output']}<ï½œendâ–ofâ–sentenceï½œ>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # å› ä¸ºeos tokenå’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„æ‰€ä»¥ è¡¥å……ä¸º1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1895f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Example Input IDs: [5726, 25, 207, 96866, 9629, 6193, 30014, 28432, 19304, 71744, 13458, 5871, 31880, 12196, 537, 34028, 6193, 79961, 30014, 19017, 6193, 12630, 7842, 33363, 30014, 11, 47296, 15190, 185, 185, 77398, 25, 207, 65766, 337, 12630, 7842, 19304, 3895, 2133, 930, 27715, 40449, 19304, 6373, 35988, 1415, 9951, 19304, 50675, 10645, 60013, 337, 56743, 1620, 26127, 2000, 6373, 27529, 2337, 224, 2084, 227, 19304, 50675, 25062, 88759, 27168, 1620, 57253, 398, 6373, 8629, 3411, 14353, 3757, 19304, 12234, 1801, 91544, 7309, 19304, 2419, 589, 110, 1363, 12933, 15409, 5394, 4142, 21824, 398, 16541, 17057, 14496, 37529, 20100, 19304, 42297, 15717, 12630, 7842, 33363, 14353, 19304, 11423, 2220, 5088, 2160, 100001, 100001]\n",
      "Labels Example: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 77398, 25, 207, 65766, 337, 12630, 7842, 19304, 3895, 2133, 930, 27715, 40449, 19304, 6373, 35988, 1415, 9951, 19304, 50675, 10645, 60013, 337, 56743, 1620, 26127, 2000, 6373, 27529, 2337, 224, 2084, 227, 19304, 50675, 25062, 88759, 27168, 1620, 57253, 398, 6373, 8629, 3411, 14353, 3757, 19304, 12234, 1801, 91544, 7309, 19304, 2419, 589, 110, 1363, 12933, 15409, 5394, 4142, 21824, 398, 16541, 17057, 14496, 37529, 20100, 19304, 42297, 15717, 12630, 7842, 33363, 14353, 19304, 11423, 2220, 5088, 2160, 100001, 100001]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(111, 111)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_example = tokenized_id[0]['input_ids']\n",
    "labels_example = tokenized_id[0]['labels']\n",
    "print(\"Tokenized Example Input IDs:\", token_example)\n",
    "print(\"Labels Example:\", labels_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7e15a0-4d9a-4935-9861-00cc472654b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Example: User: ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦æˆ‘æƒ³é€èµµè€å¸ˆç”Ÿæ—¥ç¥ç¦,ä¸¥è‚ƒé£æ ¼\n",
      "\n",
      "Assistant: å°Šæ•¬çš„èµµè€å¸ˆï¼Œå€¼æ­¤ç”Ÿè¾°ä¹‹é™…ï¼Œæ„¿å²æœˆå¦‚è¯—ï¼Œä¸ºæ‚¨å¸¦æ¥æ— å°½çš„å–œæ‚¦ä¸ç¾å¥½ï¼›æ„¿æ—¶å…‰èè‹’ï¼Œä¸ºæ‚¨ç•™ä¸‹çè´µçš„å›å¿†ä¸æ„Ÿæ‚Ÿã€‚æ„¿æ‚¨å¿«å¿«ä¹ä¹ï¼Œäº‹ä¸šæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œæ•™è¯²ä¹‹æ©æ¡ƒææ»¡å¤©ä¸‹ã€‚åœ¨è¿™å……æ»¡æ•¬æ„çš„æ—¶åˆ»ï¼Œæ­ç¥èµµè€å¸ˆç”Ÿæ—¥å¿«ä¹ï¼Œå¹¸ç¦å®‰åº·ï¼<ï½œendâ–ofâ–sentenceï½œ><ï½œendâ–ofâ–sentenceï½œ>\n",
      "Output Example: Assistant: èµµè€å¸ˆï¼Œå€¼æ­¤æ˜¥èŠ‚ä½³èŠ‚ä¹‹é™…ï¼Œæ­ç¥æ‚¨ç¦å¯¿å®‰åº·ï¼Œä¸‡äº‹å¦‚æ„ã€‚åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæ‚¨çš„è¾›å‹¤è€•è€˜ä¸ºåè¾ˆæ ‘ç«‹äº†æ¦œæ ·ï¼Œæ–°æ˜¥åˆ°æ¥ï¼Œæ„¿æ‚¨çš„ç”Ÿæ´»å¦‚è¯—å¦‚ç”»ï¼Œå·¥ä½œæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œç»§ç»­ä»¥æ‚¨çš„æ™ºæ…§å’Œçƒ­å¿±ï¼Œå¼•é¢†æˆ‘ä»¬å‰è¡Œã€‚å²æœˆé™å¥½ï¼Œæ„¿æ‚¨äº«å—æ¯ä¸€ä¸ªæ¸©é¦¨æ—¶åˆ»ï¼Œå¹¸ç¦å®‰åº·ï¼Œå–œæ‚¦æ— å¿§ã€‚<ï½œendâ–ofâ–sentenceï½œ><ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "input_example = tokenizer.decode(tokenized_id[0]['input_ids'])\n",
    "output_example = tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))\n",
    "print(\"Input Example:\", input_example)\n",
    "print(\"Output Example:\", output_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424823a8-ed0d-4309-83c8-3f6b1cdf274c",
   "metadata": {},
   "source": [
    "# 4.åˆ›å»ºæ¨¡å‹\n",
    "æ¨¡å‹ä»¥åŠç²¾åº¦å½¢å¼åŠ è½½ï¼Œå¦‚æœä½ çš„æ˜¾å¡æ¯”è¾ƒæ–°çš„è¯ï¼Œå¯ä»¥ç”¨torch.bfolatå½¢å¼åŠ è½½ã€‚å¯¹äºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸€å®šè¦æŒ‡å®štrust_remote_codeå‚æ•°ä¸ºTrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170764e5-d899-4ef4-8c53-36f6dec0d198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" \n",
    "\n",
    "model_raw = AutoModelForCausalLM.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', trust_remote_code=True, torch_dtype=torch.half, device_map=\"auto\")\n",
    "model_raw.generation_config = GenerationConfig.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/')\n",
    "model_raw.generation_config.pad_token_id = model_raw.generation_config.eos_token_id\n",
    "model_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2323eac7-37d5-4288-8bc5-79fac7113402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•\n",
    "model_raw.enable_input_require_grads()\n",
    "# æŸ¥çœ‹æ¨¡å‹çš„dtype\n",
    "model_raw.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d71257-3c1c-4303-8ff8-af161ebc2cf1",
   "metadata": {},
   "source": [
    "# 5. å®šä¹‰LoraConfig\n",
    "LoraConfigè¿™ä¸ªç±»ä¸­å¯ä»¥è®¾ç½®å¾ˆå¤šå‚æ•°ï¼Œå…·ä½“ç»†åˆ†å¯ä»¥çœ‹pertæºç \n",
    "\n",
    "- `task_type`ï¼šæ¨¡å‹ç±»å‹\n",
    "- `target_modules`ï¼šéœ€è¦è®­ç»ƒçš„æ¨¡å‹å±‚çš„åå­—ï¼Œä¸»è¦å°±æ˜¯`attention`éƒ¨åˆ†çš„å±‚ï¼Œä¸åŒçš„æ¨¡å‹å¯¹åº”çš„å±‚çš„åå­—ä¸åŒã€‚\n",
    "- `r`ï¼š`lora`çš„ç§©ï¼Œæ§åˆ¶ä½ç§©åˆ†è§£çš„ç»´åº¦\n",
    "- `lora_alpha`ï¼š`Lora alaph`ï¼Œæ§åˆ¶LoRAæƒé‡çš„ç¼©æ”¾å¼ºåº¦\n",
    "- `lora_dropout`ï¼š`Lora`çš„dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "- `inference_mode`ï¼šæ¨ç†æ¨¡å¼ï¼Œæ§åˆ¶æ˜¯å¦ä¸ºæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'up_proj', 'v_proj', 'q_proj', 'gate_proj', 'down_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # è®­ç»ƒæ¨¡å¼\n",
    "    r=8, # Lora ç§©\n",
    "    lora_alpha=32, # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†\n",
    "    lora_dropout=0.1# Dropout æ¯”ä¾‹\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2489c5-eaab-4e1f-b06a-c3f914b4bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(102400, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-29): 30 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model_raw, config)\n",
    "print(model)\n",
    "parameters = model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca055683-837f-4865-9c57-9164ba60c00f",
   "metadata": {},
   "source": [
    "# 6. é…ç½®è®­ç»ƒå‚æ•°\n",
    "TrainingArgumentsæ˜¯Hugging Face Transformersåº“ä¸­ç”¨äºé…ç½®æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒç±»ï¼Œå®ƒåŒ…å«äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§è¶…å‚æ•°å’Œè®¾ç½®ã€‚\n",
    "\n",
    "###  æ ¸å¿ƒå‚æ•°è§£é‡Š\n",
    "- `output_dir`: æ¨¡å‹å’Œè®­ç»ƒç»“æœä¿å­˜çš„ç›®å½•\n",
    "- `learning_rate`: å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¨¡å‹å‚æ•°æ›´æ–°çš„æ­¥é•¿\n",
    "- `per_device_train_batch_size`: æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "- `num_train_epochs`: è®­ç»ƒçš„æ€»è½®æ•°\n",
    "- `weight_decay`: æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "- `logging_steps`: æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”\n",
    "- `save_steps`: æ¨¡å‹ä¿å­˜çš„æ­¥æ•°é—´éš”\n",
    "- `gradient_checkpointing`: æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/DeepSeek\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec9bc36-b297-45af-99e1-d4c4d82be081",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 08:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.337800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.840900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.877100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=1.0860317428479485, metrics={'train_runtime': 527.8951, 'train_samples_per_second': 16.912, 'train_steps_per_second': 1.057, 'total_flos': 4.116561919082496e+16, 'train_loss': 1.0860317428479485, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09717e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/')\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./output/DeepSeek/checkpoint-558/\")\n",
    "merged = lora_model.merge_and_unload()\n",
    "merged.save_pretrained(\"./output/DeepSeek-wish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee043083",
   "metadata": {},
   "source": [
    "### ç»“æœæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2cee5d-3d58-4f82-8d26-0eb0158f61f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/train/llm_train/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\n",
      "\n",
      " æ„¿å²æœˆé™å¥½ï¼Œç¬‘é¥å¦‚èŠ±ğŸŒ¸ï¼Œå¹¸ç¦æ»¡æº¢ï¼Œå¿«ä¹æ— è¾¹ğŸ‰ï¼Œå§ï¼Œç”Ÿæ—¥å¿«ä¹å‘€ï¼\n"
     ]
    }
   ],
   "source": [
    "text = \"ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\"\n",
    "inputs = tokenizer(f\"User: {text}\\n\\n\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2022137",
   "metadata": {},
   "source": [
    "### åŸå§‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a399885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# åŸå§‹æ¨¡å‹\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', trust_remote_code=True, torch_dtype=torch.half, device_map=\"auto\")\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82711717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\n",
      "\n",
      "ğŸ‰ğŸ‚ğŸğŸˆğŸğŸ‚ğŸŠğŸˆğŸ‚ğŸ‰\n",
      "\n",
      "æˆ‘äº²çˆ±çš„å§å§ï¼Œä»Šå¤©æ˜¯ä½ ç‰¹åˆ«çš„ä¸€å¤©ï¼Œä½ çš„ç”Ÿæ—¥åˆ°äº†ï¼åœ¨è¿™ä¸ªç‰¹åˆ«çš„æ—¥å­é‡Œï¼Œæˆ‘æƒ³å¯¹ä½ è¯´ï¼šç¥ä½ ç”Ÿæ—¥å¿«ä¹ï¼ğŸ‰ğŸ‚ğŸğŸˆğŸ‰\n",
      "\n",
      "è®°å¾—å°æ—¶å€™ï¼Œä½ æ€»æ˜¯ä¼šç»™æˆ‘ä¹°å¥½åƒçš„ç³–æœå’Œç©å…·ï¼Œè¿˜æœ‰é‚£ä»¶æˆ‘æœ€å–œæ¬¢çš„å°çº¢è£™ï¼Œè®©æˆ‘æ„Ÿè§‰è‡ªå·±æ˜¯ä¸–ç•Œä¸Šæœ€å¹¸ç¦çš„å°å…¬ä¸»ã€‚ä½ æ€»æ˜¯é‚£ä¹ˆæ¸©æŸ”ã€ä½“è´´ï¼Œè®©æˆ‘æ„Ÿåˆ°å®‰å¿ƒå’Œæ¸©æš–ã€‚ğŸ‘¸ğŸ»ğŸ‘¸ğŸ¼ğŸ‘¸ğŸ½ğŸ‘¸ğŸ¾ğŸ‘¸ğŸ¿\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘ä¹Ÿæƒ³ç»™ä½ é€ä¸Šä¸€ä»½å°çº¢ä¹¦é£æ ¼çš„ç”Ÿæ—¥ç¥ç¦ï¼Œå¸Œæœ›ä½ èƒ½æ„Ÿå—åˆ°æˆ‘çš„çˆ±å’Œç¥ç¦ã€‚ğŸ’•\n",
      "\n",
      "ç¥ä½ æ¯å¤©éƒ½æœ‰å¥½å¿ƒæƒ…ï¼Œèº«ä½“å¥åº·ï¼Œäº‹ä¸šæœ‰æˆï¼Œå®¶åº­å¹¸ç¦ï¼Œçˆ±æƒ…ç”œèœœã€‚ğŸŒ¹\n",
      "\n",
      "æœ€åï¼Œå†æ¬¡ç¥ä½ ç”Ÿæ—¥å¿«ä¹ï¼ğŸ‰ğŸ‚ğŸğŸˆğŸ‰\n"
     ]
    }
   ],
   "source": [
    "text = \"ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\"\n",
    "inputs = tokenizer(f\"User: {text}\\n\\n\", return_tensors=\"pt\")\n",
    "outputs = deepseek_model.generate(**inputs.to(deepseek_model.device), max_new_tokens=512)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
