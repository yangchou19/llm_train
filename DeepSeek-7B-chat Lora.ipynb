{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026ce81a",
   "metadata": {},
   "source": [
    "# 1. å¯¼å…¥ç¯å¢ƒ\n",
    "\n",
    "æœ¬èŠ‚å°†å¯¼å…¥è®­ç»ƒDeepSeek-7B-chatæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰Pythonåº“å’Œä¾èµ–åŒ…ï¼Œå¹¶è¿›è¡ŒåŸºç¡€çš„ç¯å¢ƒé…ç½®ã€‚\n",
    "\n",
    "### ä¸»è¦ä¾èµ–åº“è¯´æ˜\n",
    "\n",
    "#### æ ¸å¿ƒæœºå™¨å­¦ä¹ åº“\n",
    "- **transformers**: Hugging Faceçš„Transformersåº“ï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "- **torch**: PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›æ¨¡å‹è®­ç»ƒçš„åŸºç¡€åŠŸèƒ½\n",
    "- **datasets**: Hugging Faceçš„æ•°æ®é›†å¤„ç†åº“ï¼Œç”¨äºé«˜æ•ˆå¤„ç†è®­ç»ƒæ•°æ®\n",
    "- **peft**: Parameter-Efficient Fine-Tuningåº“ï¼Œæä¾›LoRAç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•\n",
    "\n",
    "#### æ•°æ®å¤„ç†åº“\n",
    "- **pandas**: æ•°æ®åˆ†æå’Œå¤„ç†åº“ï¼Œç”¨äºè¯»å–JSONæ•°æ®å¹¶è½¬æ¢æ ¼å¼\n",
    "- **numpy**: æ•°å€¼è®¡ç®—åº“ï¼ˆé€šè¿‡å…¶ä»–åº“é—´æ¥ä½¿ç”¨ï¼‰\n",
    "\n",
    "#### è®­ç»ƒç›¸å…³ç»„ä»¶\n",
    "- **AutoTokenizer**: è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨\n",
    "- **AutoModelForCausalLM**: è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹\n",
    "- **DataCollatorForSeq2Seq**: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„æ•°æ®æ•´ç†å™¨\n",
    "- **TrainingArguments**: è®­ç»ƒå‚æ•°é…ç½®ç±»\n",
    "- **Trainer**: Hugging Faceçš„è®­ç»ƒå™¨ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹\n",
    "- **GenerationConfig**: æ–‡æœ¬ç”Ÿæˆé…ç½®ç±»\n",
    "\n",
    "### ç¯å¢ƒè¦æ±‚\n",
    "- Python 3.8+\n",
    "- CUDA 11.8+ (ç”¨äºGPUè®­ç»ƒ)\n",
    "- è‡³å°‘24GB GPUå†…å­˜ (æ¨è24GB+)\n",
    "- è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨æ¨¡å‹å’Œæ•°æ®é›†\n",
    "- GPUé©±åŠ¨å’ŒCUDAå·¥å…·åŒ…å·²æ­£ç¡®å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac949-4150-4091-b0c3-2968ab5e385c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å¯¼å…¥ç›¸åº”çš„ä¾èµ–\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d05e5d-d14e-4f03-92be-9a9677d41918",
   "metadata": {},
   "source": [
    "# 2.è¯»å–å’Œå¤„ç†æ•°æ®é›†\n",
    "### 2.1 è¯»å–æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e098d9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦',\n",
       "  'ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦',\n",
       "  'ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦'],\n",
       " 'input': ['æˆ‘æƒ³é€èµµè€å¸ˆç”Ÿæ—¥ç¥ç¦,ä¸¥è‚ƒé£æ ¼', 'é€ç¥ç¦ç»™èµµè€å¸ˆæ˜¥èŠ‚,ä¸¥è‚ƒé£æ ¼', 'ç¥èµµè€å¸ˆå…ƒå®µèŠ‚å¿«ä¹,ä¸¥è‚ƒé£æ ¼'],\n",
       " 'output': ['å°Šæ•¬çš„èµµè€å¸ˆï¼Œå€¼æ­¤ç”Ÿè¾°ä¹‹é™…ï¼Œæ„¿å²æœˆå¦‚è¯—ï¼Œä¸ºæ‚¨å¸¦æ¥æ— å°½çš„å–œæ‚¦ä¸ç¾å¥½ï¼›æ„¿æ—¶å…‰èè‹’ï¼Œä¸ºæ‚¨ç•™ä¸‹çè´µçš„å›å¿†ä¸æ„Ÿæ‚Ÿã€‚æ„¿æ‚¨å¿«å¿«ä¹ä¹ï¼Œäº‹ä¸šæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œæ•™è¯²ä¹‹æ©æ¡ƒææ»¡å¤©ä¸‹ã€‚åœ¨è¿™å……æ»¡æ•¬æ„çš„æ—¶åˆ»ï¼Œæ­ç¥èµµè€å¸ˆç”Ÿæ—¥å¿«ä¹ï¼Œå¹¸ç¦å®‰åº·ï¼',\n",
       "  'èµµè€å¸ˆï¼Œå€¼æ­¤æ˜¥èŠ‚ä½³èŠ‚ä¹‹é™…ï¼Œæ­ç¥æ‚¨ç¦å¯¿å®‰åº·ï¼Œä¸‡äº‹å¦‚æ„ã€‚åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæ‚¨çš„è¾›å‹¤è€•è€˜ä¸ºåè¾ˆæ ‘ç«‹äº†æ¦œæ ·ï¼Œæ–°æ˜¥åˆ°æ¥ï¼Œæ„¿æ‚¨çš„ç”Ÿæ´»å¦‚è¯—å¦‚ç”»ï¼Œå·¥ä½œæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œç»§ç»­ä»¥æ‚¨çš„æ™ºæ…§å’Œçƒ­å¿±ï¼Œå¼•é¢†æˆ‘ä»¬å‰è¡Œã€‚å²æœˆé™å¥½ï¼Œæ„¿æ‚¨äº«å—æ¯ä¸€ä¸ªæ¸©é¦¨æ—¶åˆ»ï¼Œå¹¸ç¦å®‰åº·ï¼Œå–œæ‚¦æ— å¿§ã€‚',\n",
       "  'å°Šæ•¬çš„èµµè€å¸ˆï¼Œå…ƒå®µä½³èŠ‚è‡³ï¼Œæ„¿æ‚¨ç¦å¯¿å®‰åº·ï¼Œæ™ºæ…§å¦‚ç¯ã€‚æ„¿è¿™å›¢åœ†çš„ç¯ç«ï¼Œç…§äº®æ‚¨çš„äº‹ä¸šä¸å‰ç¨‹ï¼Œå¸¦æ¥æ›´å¤šå­¦æœ¯çš„è¾‰ç…Œä¸æˆå°±ã€‚åœ¨è¿™æ¸©é¦¨çš„æ—¶åˆ»ï¼Œæ„¿æ‚¨å®¶åº­ç¾æ»¡ï¼Œå¹¸ç¦é•¿å­˜ï¼Œå¦‚å…ƒå®µèˆ¬åœ†æ»¡ç”œèœœã€‚è°¨ç¥å…ƒå®µå¿«ä¹ï¼Œä¸‡äº‹å¦‚æ„ï¼']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å–å¯¹åº”çš„æ•°æ®\n",
    "file_path = './dataset/tianji-chinese/tianji-wishes-chinese-v0.1-format.json'\n",
    "df = pd.read_json(file_path)\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccb2c5",
   "metadata": {},
   "source": [
    "### 2.2 åŠ è½½åˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./deepseek-ai/deepseek-llm-7b-chat/', vocab_size=100000, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<ï½œbeginâ–ofâ–sentenceï½œ>', 'eos_token': '<ï½œendâ–ofâ–sentenceï½œ>', 'pad_token': '<ï½œendâ–ofâ–sentenceï½œ>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t100000: AddedToken(\"<ï½œbeginâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t100001: AddedToken(\"<ï½œendâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t100002: AddedToken(\"Ã¸\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100003: AddedToken(\"Ã¶\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100004: AddedToken(\"Ãº\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100005: AddedToken(\"Ã¿\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100006: AddedToken(\"Ãµ\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100007: AddedToken(\"Ã·\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100008: AddedToken(\"Ã»\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100009: AddedToken(\"Ã½\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100010: AddedToken(\"Ã€\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100011: AddedToken(\"Ã¹\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100012: AddedToken(\"Ã\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100013: AddedToken(\"Ã¾\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t100014: AddedToken(\"Ã¼\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', use_fast=False, trust_remote_code=True)\n",
    "# è®¾ç½®å¡«å……æ–¹å‘ä¸ºå³ä¾§å¡«å……ï¼Œä¸ä¼šå½±å“æ¨¡å‹å¯¹åºåˆ—å¼€å¤´çš„ç†è§£\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c330e94",
   "metadata": {},
   "source": [
    "### 2.3 æ•°æ®æ ¼å¼åŒ–\n",
    "Lora è®­ç»ƒçš„æ•°æ®æ˜¯éœ€è¦ç»è¿‡æ ¼å¼åŒ–ã€ç¼–ç ä¹‹åå†è¾“å…¥ç»™æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå°†æ•°æ®ç¼–ç æˆå¤šç»´å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2503a5fa-9621-4495-9035-8e7ef6525691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2976/2976 [00:01<00:00, 2236.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2976\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llamaåˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"User: {example['instruction']+example['input']}\\n\\n\", add_special_tokens=False)  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens\n",
    "    response = tokenizer(f\"Assistant: {example['output']}<ï½œendâ–ofâ–sentenceï½œ>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    # print(\"instruction\", instruction)\n",
    "    # print(\"response\", response)\n",
    "    # print(\"input_ids\", input_ids)\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # å› ä¸ºeos tokenå’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„æ‰€ä»¥ è¡¥å……ä¸º1\n",
    "    # print(\"attention_mask\", attention_mask)\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f7e15a0-4d9a-4935-9861-00cc472654b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Example: User: ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé€ç¥ç¦å¤§å¸ˆï¼Œå¸®æˆ‘é’ˆå¯¹ä¸åŒäººå’Œäº‹æƒ…ã€èŠ‚æ—¥é€å¯¹åº”çš„ç¥ç¦æˆ‘æƒ³é€èµµè€å¸ˆç”Ÿæ—¥ç¥ç¦,ä¸¥è‚ƒé£æ ¼\n",
      "\n",
      "Assistant: å°Šæ•¬çš„èµµè€å¸ˆï¼Œå€¼æ­¤ç”Ÿè¾°ä¹‹é™…ï¼Œæ„¿å²æœˆå¦‚è¯—ï¼Œä¸ºæ‚¨å¸¦æ¥æ— å°½çš„å–œæ‚¦ä¸ç¾å¥½ï¼›æ„¿æ—¶å…‰èè‹’ï¼Œä¸ºæ‚¨ç•™ä¸‹çè´µçš„å›å¿†ä¸æ„Ÿæ‚Ÿã€‚æ„¿æ‚¨å¿«å¿«ä¹ä¹ï¼Œäº‹ä¸šæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œæ•™è¯²ä¹‹æ©æ¡ƒææ»¡å¤©ä¸‹ã€‚åœ¨è¿™å……æ»¡æ•¬æ„çš„æ—¶åˆ»ï¼Œæ­ç¥èµµè€å¸ˆç”Ÿæ—¥å¿«ä¹ï¼Œå¹¸ç¦å®‰åº·ï¼<ï½œendâ–ofâ–sentenceï½œ><ï½œendâ–ofâ–sentenceï½œ>\n",
      "Output Example: Assistant: èµµè€å¸ˆï¼Œå€¼æ­¤æ˜¥èŠ‚ä½³èŠ‚ä¹‹é™…ï¼Œæ­ç¥æ‚¨ç¦å¯¿å®‰åº·ï¼Œä¸‡äº‹å¦‚æ„ã€‚åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæ‚¨çš„è¾›å‹¤è€•è€˜ä¸ºåè¾ˆæ ‘ç«‹äº†æ¦œæ ·ï¼Œæ–°æ˜¥åˆ°æ¥ï¼Œæ„¿æ‚¨çš„ç”Ÿæ´»å¦‚è¯—å¦‚ç”»ï¼Œå·¥ä½œæ›´ä¸Šä¸€å±‚æ¥¼ï¼Œç»§ç»­ä»¥æ‚¨çš„æ™ºæ…§å’Œçƒ­å¿±ï¼Œå¼•é¢†æˆ‘ä»¬å‰è¡Œã€‚å²æœˆé™å¥½ï¼Œæ„¿æ‚¨äº«å—æ¯ä¸€ä¸ªæ¸©é¦¨æ—¶åˆ»ï¼Œå¹¸ç¦å®‰åº·ï¼Œå–œæ‚¦æ— å¿§ã€‚<ï½œendâ–ofâ–sentenceï½œ><ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "input_example = tokenizer.decode(tokenized_id[0]['input_ids'])\n",
    "output_example = tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))\n",
    "print(\"Input Example:\", input_example)\n",
    "print(\"Output Example:\", output_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424823a8-ed0d-4309-83c8-3f6b1cdf274c",
   "metadata": {},
   "source": [
    "# 4.åˆ›å»ºæ¨¡å‹\n",
    "æ¨¡å‹ä»¥åŠç²¾åº¦å½¢å¼åŠ è½½ï¼Œå¦‚æœä½ çš„æ˜¾å¡æ¯”è¾ƒæ–°çš„è¯ï¼Œå¯ä»¥ç”¨torch.bfolatå½¢å¼åŠ è½½ã€‚å¯¹äºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸€å®šè¦æŒ‡å®štrust_remote_codeå‚æ•°ä¸ºTrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "170764e5-d899-4ef4-8c53-36f6dec0d198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', trust_remote_code=True, torch_dtype=torch.half, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/')\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2323eac7-37d5-4288-8bc5-79fac7113402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•\n",
    "model.enable_input_require_grads()\n",
    "# æŸ¥çœ‹æ¨¡å‹çš„dtype\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d71257-3c1c-4303-8ff8-af161ebc2cf1",
   "metadata": {},
   "source": [
    "# 5. å®šä¹‰LoraConfig\n",
    "LoraConfigè¿™ä¸ªç±»ä¸­å¯ä»¥è®¾ç½®å¾ˆå¤šå‚æ•°ï¼Œä½†ä¸»è¦çš„å‚æ•°æ²¡å¤šå°‘ï¼Œç®€å•è®²ä¸€è®²ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç›´æ¥çœ‹æºç ã€‚\n",
    "\n",
    "- `task_type`ï¼šæ¨¡å‹ç±»å‹\n",
    "- `target_modules`ï¼šéœ€è¦è®­ç»ƒçš„æ¨¡å‹å±‚çš„åå­—ï¼Œä¸»è¦å°±æ˜¯`attention`éƒ¨åˆ†çš„å±‚ï¼Œä¸åŒçš„æ¨¡å‹å¯¹åº”çš„å±‚çš„åå­—ä¸åŒï¼Œå¯ä»¥ä¼ å…¥æ•°ç»„ï¼Œä¹Ÿå¯ä»¥å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ­£åˆ™è¡¨è¾¾å¼ã€‚\n",
    "- `r`ï¼š`lora`çš„ç§©ï¼Œæ§åˆ¶ä½ç§©åˆ†è§£çš„ç»´åº¦\n",
    "- `lora_alpha`ï¼š`Lora alaph`ï¼Œæ§åˆ¶LoRAæƒé‡çš„ç¼©æ”¾å¼ºåº¦\n",
    "- `lora_dropout`ï¼š`Lora`çš„dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "- `inference_mode`ï¼šæ¨ç†æ¨¡å¼ï¼Œæ§åˆ¶æ˜¯å¦ä¸ºæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'k_proj', 'up_proj', 'o_proj', 'gate_proj', 'down_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # è®­ç»ƒæ¨¡å¼\n",
    "    r=8, # Lora ç§©\n",
    "    lora_alpha=32, # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†\n",
    "    lora_dropout=0.1# Dropout æ¯”ä¾‹\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2489c5-eaab-4e1f-b06a-c3f914b4bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(102400, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-29): 30 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n",
      "Trainable parameters: None\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "print(model)\n",
    "parameters = model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca055683-837f-4865-9c57-9164ba60c00f",
   "metadata": {},
   "source": [
    "# 6. é…ç½®è®­ç»ƒå‚æ•°\n",
    "TrainingArgumentsæ˜¯Hugging Face Transformersåº“ä¸­ç”¨äºé…ç½®æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒç±»ï¼Œå®ƒåŒ…å«äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§è¶…å‚æ•°å’Œè®¾ç½®ã€‚\n",
    "\n",
    "### æ ¸å¿ƒå‚æ•°è§£é‡Š\n",
    "- `output_dir`: æ¨¡å‹å’Œè®­ç»ƒç»“æœä¿å­˜çš„ç›®å½•\n",
    "- `learning_rate`: å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¨¡å‹å‚æ•°æ›´æ–°çš„æ­¥é•¿\n",
    "- `per_device_train_batch_size`: æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "- `num_train_epochs`: è®­ç»ƒçš„æ€»è½®æ•°\n",
    "- `weight_decay`: æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "- `logging_steps`: æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”\n",
    "- `save_steps`: æ¨¡å‹ä¿å­˜çš„æ­¥æ•°é—´éš”\n",
    "- `gradient_checkpointing`: æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/DeepSeek\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aec9bc36-b297-45af-99e1-d4c4d82be081",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 08:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.997300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=1.058647850508331, metrics={'train_runtime': 522.8689, 'train_samples_per_second': 17.075, 'train_steps_per_second': 1.067, 'total_flos': 4.116561919082496e+16, 'train_loss': 1.058647850508331, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6c151",
   "metadata": {},
   "source": [
    "# 7. æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b2cee5d-3d58-4f82-8d26-0eb0158f61f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\n",
      "\n",
      "å°Šæ•¬çš„å§å§ï¼Œä»Šå¤©æ˜¯ä½ çš„ç”Ÿæ—¥ï¼Œæˆ‘ç¥ä½ ç”Ÿæ—¥å¿«ä¹ï¼åœ¨è¿™ä¸ªç‰¹åˆ«çš„æ—¥å­é‡Œï¼Œæˆ‘æƒ³å¯¹ä½ è¯´ï¼Œä½ æ˜¯æˆ‘ç”Ÿå‘½ä¸­æœ€é‡è¦çš„äººä¹‹ä¸€ï¼Œä½ çš„å­˜åœ¨è®©æˆ‘çš„ç”Ÿæ´»å˜å¾—æ›´åŠ ç¾å¥½ã€‚æˆ‘æ„Ÿè°¢ä½ ä¸€ç›´ä»¥æ¥å¯¹æˆ‘çš„å…³å¿ƒå’Œå¸®åŠ©ï¼Œä½ çš„æ”¯æŒå’Œé¼“åŠ±æ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›ã€‚\n",
      "\n",
      "ç¥ä½ ç”Ÿæ—¥å¿«ä¹ï¼æ„¿ä½ çš„ç”Ÿæ´»å……æ»¡é˜³å…‰ã€é²œèŠ±å’Œå¿«ä¹ï¼Œæ„¿ä½ æ‹¥æœ‰å¥åº·ã€å¹¸ç¦å’ŒæˆåŠŸã€‚å¸Œæœ›ä½ åœ¨æœªæ¥çš„æ—¥å­é‡Œï¼Œèƒ½å¤Ÿå®ç°è‡ªå·±çš„æ¢¦æƒ³ï¼Œè¿½æ±‚è‡ªå·±çš„å¹¸ç¦ã€‚\n",
      "\n",
      "æœ€å\n"
     ]
    }
   ],
   "source": [
    "# text = \"å°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€”\"\n",
    "text = \"ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\"\n",
    "inputs = tokenizer(f\"User: {text}\\n\\n\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a399885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "deepseek_model = AutoModelForCausalLM.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/', trust_remote_code=True, torch_dtype=torch.half, device_map=\"auto\")\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('./deepseek-ai/deepseek-llm-7b-chat/')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "82711717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\n",
      "\n",
      "ğŸ‰ğŸ‚ğŸğŸğŸğŸ‚ğŸ‰\n",
      "\n",
      "äº²çˆ±çš„å°çº¢ä¹¦å®å®ä»¬ï¼Œä»Šå¤©æˆ‘è¦å’Œå¤§å®¶åˆ†äº«ä¸€ä»¶ç‰¹åˆ«å¼€å¿ƒçš„äº‹æƒ…ï¼Œæˆ‘çš„å§å§ä»Šå¤©è¿‡ç”Ÿæ—¥å•¦ï¼ğŸ‰ğŸ‚ğŸˆ\n",
      "\n",
      "å§å§æ˜¯æˆ‘ç”Ÿå‘½ä¸­æœ€é‡è¦çš„äººä¹‹ä¸€ï¼Œå¥¹ä¸ä»…æ˜¯æˆ‘æœ€äº²çš„äº²äººï¼Œä¹Ÿæ˜¯æˆ‘æœ€å¯é çš„æœ‹å‹ã€‚è®°å¾—å°æ—¶å€™ï¼Œæ¯æ¬¡æˆ‘é‡åˆ°å›°éš¾å’ŒæŒ«æŠ˜ï¼Œå§å§æ€»æ˜¯ç¬¬ä¸€ä¸ªç«™å‡ºæ¥æ”¯æŒæˆ‘ï¼Œé¼“åŠ±æˆ‘ï¼Œå¸®åŠ©æˆ‘ã€‚å¥¹çš„ç¬‘å®¹ï¼Œå¥¹çš„æ¸©æš–ï¼Œä¸€ç›´æ˜¯æˆ‘ç”Ÿå‘½ä¸­æœ€ç¾å¥½çš„å›å¿†ã€‚ğŸ¥°\n",
      "\n",
      "æ‰€ä»¥ä»Šå¤©ï¼Œæˆ‘æƒ³é€šè¿‡å°çº¢ä¹¦è¿™ä¸ªå¹³å°ï¼Œå‘å…¨ä¸–ç•Œå¤§å£°åœ°è¯´ï¼šå§å§ï¼Œç”Ÿæ—¥å¿«ä¹ï¼å¸Œæœ›ä½ çš„æ¯ä¸€å¤©éƒ½èƒ½åƒä»Šå¤©ä¸€æ ·ï¼Œå……æ»¡æ¬¢ç¬‘å’Œå¹¸ç¦ã€‚ğŸ‰ğŸ‚ğŸˆ\n",
      "\n",
      "å½“ç„¶ï¼Œä½œä¸ºä½ çš„äº²å¦¹å¦¹ï¼Œæˆ‘ä¹Ÿè¦é€ä¸Šä¸€ä»½ç‰¹åˆ«çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œå¸Œæœ›ä½ èƒ½å¤Ÿå–œæ¬¢ã€‚ä¸è¿‡ï¼Œåœ¨è¿™é‡Œæˆ‘å°±ä¸å‰§é€å•¦ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æœŸå¾…å§å§æ‹†å¼€ç¤¼ç‰©çš„é‚£ä¸€åˆ»å§ï¼ğŸğŸğŸ\n",
      "\n",
      "æœ€åï¼Œæˆ‘æƒ³ç”¨ä¸€å¥è¯æ¥æ€»ç»“æˆ‘å¯¹å§å§çš„ç¥ç¦ï¼šæ„¿ä½ çš„æ¯ä¸€å¤©éƒ½å……æ»¡çˆ±å’Œå¿«ä¹ï¼Œç”Ÿæ—¥å¿«ä¹ï¼ğŸ‰ğŸ‚ğŸ‰\n",
      "\n",
      "#ç”Ÿæ—¥å¿«ä¹ #å§å§ #å°çº¢ä¹¦ #åˆ†äº« #ç¥ç¦\n"
     ]
    }
   ],
   "source": [
    "text = \"ç¥å§å§ç”Ÿæ—¥å¿«ä¹, å°çº¢ä¹¦é£æ ¼\"\n",
    "inputs = tokenizer(f\"User: {text}\\n\\n\", return_tensors=\"pt\")\n",
    "outputs = deepseek_model.generate(**inputs.to(deepseek_model.device), max_new_tokens=512)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
